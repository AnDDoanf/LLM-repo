{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMx4M5OpL20oPL6dFreoAzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e63a8f3556a24146ad8048823f571979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35658de94f284d7da44eab47ced95cec",
              "IPY_MODEL_44474faa45744c6686555446b43f23db",
              "IPY_MODEL_13c32462e44b47b0815187d3eb3494b4"
            ],
            "layout": "IPY_MODEL_5b7c3b95e1354436b7253d7ab64f3d22"
          }
        },
        "35658de94f284d7da44eab47ced95cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9024b53f17bb448e96fffc9460e86371",
            "placeholder": "​",
            "style": "IPY_MODEL_e239432fe23d4a9d925c095f6454dcd2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "44474faa45744c6686555446b43f23db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74dc4c551e544d7599ecc780f017a9dc",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c2185b718dd48a7b43a7e595bc996ba",
            "value": 3
          }
        },
        "13c32462e44b47b0815187d3eb3494b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b450facc058a470fb1d2231ac414473a",
            "placeholder": "​",
            "style": "IPY_MODEL_2083b1f0bd8d4ac491715fc24c467e93",
            "value": " 3/3 [02:34&lt;00:00, 48.97s/it]"
          }
        },
        "5b7c3b95e1354436b7253d7ab64f3d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9024b53f17bb448e96fffc9460e86371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e239432fe23d4a9d925c095f6454dcd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74dc4c551e544d7599ecc780f017a9dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c2185b718dd48a7b43a7e595bc996ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b450facc058a470fb1d2231ac414473a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2083b1f0bd8d4ac491715fc24c467e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnDDoanf/LLM-repo/blob/master/multi_prompts_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PCj5VfPLz7Qi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_community bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from google.colab import userdata\n",
        "\n",
        "class LLMConfig:\n",
        "  def __init__(self):\n",
        "    self.model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "    self.device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "    self.hf_auth = userdata.get('HF_TOKEN')\n",
        "    self.task = 'text-generation'\n",
        "    self.temperature = 1\n",
        "    self.max_new_tokens = 512\n",
        "    self.repetition_penalty = 1.2\n",
        "\n",
        "class BuildLLM:\n",
        "  def __init__(self) -> None:\n",
        "    self.config = LLMConfig()\n",
        "    model_id = self.config.model_id\n",
        "    device = self.config.device\n",
        "    hf_auth = self.config.hf_auth\n",
        "\n",
        "    bnb_config = transformers.BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=bfloat16\n",
        "    )\n",
        "\n",
        "    model_config = transformers.AutoConfig.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "        config=model_config,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map='auto',\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    generate_text = transformers.pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=True,\n",
        "        task=self.config.task,\n",
        "        temperature=self.config.temperature,\n",
        "        max_new_tokens=self.config.max_new_tokens,\n",
        "        repetition_penalty=self.config.repetition_penalty\n",
        "    )\n",
        "\n",
        "    self.llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "  def get_llm(self):\n",
        "    return self.llm\n",
        "\n",
        "  def get_chain(self, prompt):\n",
        "    return RunnableSequence(prompt | self.llm)"
      ],
      "metadata": {
        "id": "kps-FlLYi1Lz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "build_llm = BuildLLM()\n",
        "llm = build_llm.get_llm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "e63a8f3556a24146ad8048823f571979",
            "35658de94f284d7da44eab47ced95cec",
            "44474faa45744c6686555446b43f23db",
            "13c32462e44b47b0815187d3eb3494b4",
            "5b7c3b95e1354436b7253d7ab64f3d22",
            "9024b53f17bb448e96fffc9460e86371",
            "e239432fe23d4a9d925c095f6454dcd2",
            "74dc4c551e544d7599ecc780f017a9dc",
            "7c2185b718dd48a7b43a7e595bc996ba",
            "b450facc058a470fb1d2231ac414473a",
            "2083b1f0bd8d4ac491715fc24c467e93"
          ]
        },
        "id": "tKrTjtBYiyEt",
        "outputId": "5b770f94-3e24-43f3-f63a-1c7c152fa1fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:919: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e63a8f3556a24146ad8048823f571979"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langchain.chains.router.llm_router import RouterOutputParser\n",
        "from langchain_core.output_parsers.base import BaseLLMOutputParser\n",
        "\n",
        "class CustomLLMOutputParser(BaseLLMOutputParser):\n",
        "    def parse_result(self, result: list) -> str:\n",
        "        text = result[0].text\n",
        "        cleaned_text = text[text.rfind(\"[/INST]\")+7:]\n",
        "        return cleaned_text\n",
        "\n",
        "class CustomRouterOutputParser(RouterOutputParser):\n",
        "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
        "        processed_text = \"```json\\n\"+text[text.rfind('{'):].strip()\n",
        "        return super().parse(processed_text)"
      ],
      "metadata": {
        "id": "SJIOEDSFrsIt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "word_limit = 200\n",
        "\n",
        "physics_template = B_INST + f\"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "Please answer in {word_limit} words or less.\n",
        "DO answer formally.\n",
        "\n",
        "Please answer the question:\n",
        "\"\"\" + \"{input}\" + E_INST\n",
        "\n",
        "\n",
        "math_template = B_INST + f\"\"\"You are a very good mathematician.\n",
        "Your goal is to answer math questions from user.\n",
        "DO break down hard problems into their component parts, answer the component parts, then put them together to answer the broader question.\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "DO NOT add anything else.\n",
        "Please answer in {word_limit} words or less.\n",
        "DO answer formally.\n",
        "\n",
        "Please solve this problem:\n",
        "\"\"\" + \"{input}\" + E_INST\n",
        "\n",
        "history_template = B_INST + f\"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "Please answer in {word_limit} words or less.\n",
        "DO answer formally.\n",
        "\n",
        "Here is a question:\n",
        "\"\"\" + \"{input}\" + E_INST\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"history\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "ijRsVWPqkZ0X"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt, output_parser=CustomLLMOutputParser())\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "WtZ_wMnHlEbo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a description of what the prompt is best suited for.\n",
        "You may also summarize the original input if you think that summerizing it will ultimately lead to a better response from the language model.\n",
        "DO NOT revise the question.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "UkITn9sRlGd5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=CustomRouterOutputParser(partial=True),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "5i72x_1OlPdZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "SI7kBODElRJY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "chain.invoke(\"What are 3 laws of Newton?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOWSAjPinKOJ",
        "outputId": "7c422f2b-faf2-4f94-e3b0-4746186fd0a4"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': \"Newton's three laws of motion\"}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"Newton's three laws of motion\",\n",
              " 'text': \"  Certainly! Newton's Three Laws of Motion are fundamental principles that describe how objects move and respond to forces. Here is a brief overview of each law, stated as clearly and concisely as possible:\\n\\nFirst Law (Law of Inertia): An object at rest remains at rest, and an object in motion remains in motion, unless acted upon by an external force. This means that if there are no net forces acting on an object, it will maintain its current state of motion.\\n\\nSecond Law (Law of Acceleration): The acceleration of an object is directly proportional to the net force acting upon it, and inversely proportional to its mass. This means that the greater the force applied to an object, the more it will accelerate, but the more massive the object, the less it will accelerate.\\n\\nThird Law (Law of Action and Reaction): For every action, there is an equal and opposite reaction. This means that when one object exerts a force on another object, the second object will always experience an equal and opposite force.\\n\\nI hope this helps! Do you have any further questions?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}