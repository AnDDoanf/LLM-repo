{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnDDoanf/LLM-repo/blob/master/math_agent_mistrral7b_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!huggingface-cli login --token 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"
      ],
      "metadata": {
        "id": "yO-bL2PEXjZj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain transformers langchain_community bitsandbytes accelerate\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from langchain import LLMChain, PromptTemplate, HuggingFacePipeline\n",
        "import sympy as sp"
      ],
      "metadata": {
        "id": "PYVCju8TYmy5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "from torch import cuda, bfloat16\n",
        "\n",
        "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=300,\n",
        ")\n",
        "\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
      ],
      "metadata": {
        "id": "B-VMNi5FZVUG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a math expert. Solve the following math problem and provide a detailed explanation:\n",
        "\n",
        "Problem: {query}\n",
        "\n",
        "Solution:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n",
        "# Define the LLMChain\n",
        "llm_chain = LLMChain(\n",
        "    llm=mistral_llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "HWr6GzyjbDMF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process math queries (unchanged)\n",
        "def process_math_query(query):\n",
        "    # Generate response using LLMChain\n",
        "    response = llm_chain.run({\"query\": query})\n",
        "\n",
        "    # Use sympy for actual math processing if needed\n",
        "    try:\n",
        "        result = sp.sympify(query)\n",
        "        response += f\"\\nSymPy Result: {result}\"\n",
        "    except Exception as e:\n",
        "        response += f\"\\nSymPy Error: {e}\"\n",
        "\n",
        "    return response\n",
        "\n",
        "# Continuous processing loop (unchanged)\n",
        "while True:\n",
        "    user_input = input(\"Enter a math query: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    result = process_math_query(user_input)\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHmEFahUYhXf",
        "outputId": "92ca70d8-2079-4984-ad0b-31a81b17a362"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a math query: What is the largest number of these: 1, 14, 7, 11\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are a math expert. Solve the following math problem and provide a detailed explanation:\n",
            "\n",
            "Problem: What is the largest number of these: 1, 14, 7, 11\n",
            "\n",
            "Solution:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "You are a math expert. Solve the following math problem and provide a detailed explanation:\n",
            "\n",
            "Problem: What is the largest number of these: 1, 14, 7, 11\n",
            "\n",
            "Solution:\n",
            "To determine the largest number among 1, 14, 7, and 11, we need to compare them. We can do this by looking at their digits and comparing the highest digit from left to right.\n",
            "\n",
            "Starting with the highest digit, we see that all four numbers have a 1 in the tens place. Therefore, we cannot use this digit to determine the largest number.\n",
            "\n",
            "Moving on to the next digit, we see that 14 has a 4 in the units place, which is greater than the 1 in 1, 7, and 11. This means that 14 is the largest number among these four options.\n",
            "SymPy Error: Sympify of expression 'could not parse 'What is the largest number of these: 1, 14, 7, 11'' failed, because of exception being raised:\n",
            "SyntaxError: invalid syntax (<string>, line 1)\n",
            "Enter a math query: How much do you need to add to 2 to get 5?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are a math expert. Solve the following math problem and provide a detailed explanation:\n",
            "\n",
            "Problem: How much do you need to add to 2 to get 5?\n",
            "\n",
            "Solution:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "You are a math expert. Solve the following math problem and provide a detailed explanation:\n",
            "\n",
            "Problem: How much do you need to add to 2 to get 5?\n",
            "\n",
            "Solution:\n",
            "To solve this problem, we simply subtract 2 from both sides of the equation. This gives us:\n",
            "\n",
            "3 = 5 - 2\n",
            "\n",
            "Explanation:\n",
            "The left side of the equation represents the number we want to find (in this case, how much we need to add to 2 to get 5). The right side of the equation represents the result of subtracting 2 from 5. By subtracting 2 from both sides, we can isolate the number we want to find on one side of the equation. In this case, we found that we need to add 3 to 2 to get 5.\n",
            "SymPy Error: Sympify of expression 'could not parse 'How much do you need to add to 2 to get 5?'' failed, because of exception being raised:\n",
            "SyntaxError: invalid syntax (<string>, line 1)\n",
            "Enter a math query: quit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP5NCjD5lP7YLFHB/JexxfF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}