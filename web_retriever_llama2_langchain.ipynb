{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoVOtCbJVJGGUZ6iMzlgzp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnDDoanf/LLM-repo/blob/master/web_retriever_llama2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain torch transformers faiss-gpu bitsandbytes accelerate langchain-community==0.2.1 langchain-core==0.2.1 llama-index-embeddings-langchain llama-index  sentence-transformers langchain_google_community html2text gradio"
      ],
      "metadata": {
        "id": "WwQiAnc_OWqW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5Xz6jXvxLuYy"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.retrievers.web_research import WebResearchRetriever\n",
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "import os\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "import gradio as gr\n",
        "\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore import InMemoryDocstore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCzRX6KLC-Vz1Evduq8bAWLiZ_hlyOgqqs\"\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = \"d793d7e247bc848a7\""
      ],
      "metadata": {
        "id": "cAzGHlHrLxHl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "class LLMConfig:\n",
        "  def __init__(self):\n",
        "    self.model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "    self.device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "    self.hf_auth = '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'\n",
        "    self.task = 'text-generation'\n",
        "    self.temperature = 1\n",
        "    self.max_new_tokens = 512\n",
        "    self.repetition_penalty = 1.2\n",
        "\n",
        "class BuildLLM:\n",
        "  def __init__(self) -> None:\n",
        "    self.config = LLMConfig()\n",
        "    model_id = self.config.model_id\n",
        "    device = self.config.device\n",
        "    hf_auth = self.config.hf_auth\n",
        "\n",
        "    bnb_config = transformers.BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=bfloat16\n",
        "    )\n",
        "\n",
        "    model_config = transformers.AutoConfig.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "        config=model_config,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map='auto',\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=hf_auth\n",
        "    )\n",
        "\n",
        "    generate_text = transformers.pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=True,\n",
        "        task=self.config.task,\n",
        "        temperature=self.config.temperature,\n",
        "        max_new_tokens=self.config.max_new_tokens,\n",
        "        repetition_penalty=self.config.repetition_penalty\n",
        "    )\n",
        "\n",
        "    self.llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "  def get_llm(self):\n",
        "    return self.llm"
      ],
      "metadata": {
        "id": "TDmQLJDhOUyv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "builder = BuildLLM()\n",
        "llm = builder.get_llm()\n",
        "\n",
        "lc_embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        ")"
      ],
      "metadata": {
        "id": "O89wdQ3SQOlN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "def settings():\n",
        "    # Vectorstore\n",
        "    embeddings_model = LangchainEmbedding(lc_embed_model)\n",
        "    embedding_size = len(embeddings_model.get_text_embedding(\"test\"))\n",
        "    # print(f\"Embedding size: {embedding_size}\")\n",
        "    index = faiss.IndexFlatL2(embedding_size)\n",
        "    vectorstore_public = FAISS(embeddings_model.get_text_embedding, index, InMemoryDocstore({}), {})\n",
        "\n",
        "    # Search\n",
        "    from langchain_google_community import GoogleSearchAPIWrapper\n",
        "    search = GoogleSearchAPIWrapper()\n",
        "\n",
        "    # Initialize\n",
        "    web_retriever = WebResearchRetriever.from_llm(\n",
        "        vectorstore=vectorstore_public,\n",
        "        llm=llm,\n",
        "        search=search,\n",
        "        num_search_results=3\n",
        "    )\n",
        "\n",
        "    return web_retriever, llm"
      ],
      "metadata": {
        "id": "kgA5lhtQMFZt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make retriever and llm\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "web_retriever, llm = settings()\n",
        "\n",
        "# User input\n",
        "question = \"what is love\"\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, please think rationally and answer from your own knowledge base.\n",
        "{context}\n",
        "{summaries}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\", \"summaries\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=web_retriever, chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LEUkrt0QWB7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "\n",
        "header = \"Interweb Explorer\"\n",
        "info = \"I am an AI that can answer questions by exploring, reading, and summarizing web pages. I can be configured to use different modes: public API or private (no data sharing).\"\n",
        "\n",
        "inputs = gr.Textbox(label=\"Ask a question:\")\n",
        "answer_box = gr.Textbox(label=\"Answer\")\n",
        "sources_box = gr.Textbox(label=\"Sources\")\n",
        "source_documents_box = gr.Textbox(label=\"Source Documents\")\n",
        "outputs = [\n",
        "    answer_box,\n",
        "    sources_box,\n",
        "    source_documents_box\n",
        "]\n",
        "\n",
        "def get_answer(question):\n",
        "  result = qa_chain({\"question\": question, \"context\": question, \"summaries\": None})\n",
        "  return {\n",
        "      answer_box: result[\"answer\"],\n",
        "      sources_box: result[\"sources\"],\n",
        "      source_documents_box: \"\\n\".join(list(set([item.metadata['source'] for item in result[\"source_documents\"]])))\n",
        "  }\n",
        "\n",
        "iface = gr.Interface(fn=get_answer, inputs=inputs, outputs=outputs, title=header, description=info)\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "WmLBaPxaeoZy",
        "outputId": "dda39aff-5e70-4980-8d09-75500ad5bb23"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e96206d1f3b1350072.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e96206d1f3b1350072.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}
