{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtueBfLDfNkSsbYOi0K85V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnDDoanf/LLM-repo/blob/master/RAG_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "others: \"https://edersoncorbari.github.io/friends/\""
      ],
      "metadata": {
        "id": "Qdb0yrarnXcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Retriever from website"
      ],
      "metadata": {
        "id": "QmI6U6CAyzsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://edersoncorbari.github.io/friends-scripts/\"\n",
        "url_paths = []\n",
        "res = requests.get(url).content\n",
        "soup = BeautifulSoup(res, 'html.parser')\n",
        "for path in soup.find_all('a', href=True):\n",
        "  url_paths.append(url+path['href'])"
      ],
      "metadata": {
        "id": "bFQkYgRynQnY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain html2text sentence-transformers faiss-gpu langchain-community playwright langchain-huggingface pypdf\n",
        "!playwright install\n",
        "\n",
        "from typing import List\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "import nest_asyncio\n",
        "import os\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "class UltimateRAG:\n",
        "  def __init__(self, chunk_size, chunk_overlap) -> None:\n",
        "    self.chunk_size = chunk_size\n",
        "    self.chunk_overlap = chunk_overlap\n",
        "    self.text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\\n\",\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap = chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex= False)\n",
        "\n",
        "  def url_text_loader(self, url_paths):\n",
        "    if url_paths == []: return []\n",
        "    loader = AsyncChromiumLoader(url_paths)\n",
        "    docs = loader.load()\n",
        "    html2text = Html2TextTransformer()\n",
        "    documents = html2text.transform_documents(docs)\n",
        "    chunked_documents = self.text_splitter.split_documents(documents)\n",
        "    print(f\"DEBUG:\", chunked_documents[0])\n",
        "    return chunked_documents\n",
        "\n",
        "  def txt_text_loader(self, dir_path):\n",
        "    if dir_path == \"\": return []\n",
        "    documents = []\n",
        "    for file in os.listdir(dir_path):\n",
        "        if file.endswith(\".txt\"):\n",
        "            filepath = os.path.join(dir_path, file)\n",
        "            with open(filepath, 'r') as f:\n",
        "                documents.append(Document(page_content=f.read()))\n",
        "\n",
        "    chunked_documents = self.text_splitter.split_documents(documents)\n",
        "    print(f\"DEBUG:\", chunked_documents[0])\n",
        "    return chunked_documents\n",
        "\n",
        "  def pdf_text_loader(self, dir_path):\n",
        "    if dir_path == \"\": return []\n",
        "    documents = []\n",
        "    for file in os.listdir(dir_path):\n",
        "      if file.endswith(\".pdf\"):\n",
        "        filepath = os.path.join(dir_path, file)\n",
        "        loader = PyPDFLoader(filepath)\n",
        "        documents += loader.load()\n",
        "    chunked_documents = self.text_splitter.split_documents(documents)\n",
        "    print(f\"DEBUG:\", chunked_documents[0])\n",
        "    return chunked_documents\n",
        "\n",
        "  def create_retriever(self, url_paths=[], dir_path=\"\"):\n",
        "    documents_from_url = self.url_text_loader(url_paths)\n",
        "    documents_from_txt = self.txt_text_loader(dir_path)\n",
        "    documents_from_pdf = self.pdf_text_loader(dir_path)\n",
        "    chunked_documents = documents_from_txt + documents_from_pdf + documents_from_url\n",
        "\n",
        "    db = FAISS.from_documents(\n",
        "        chunked_documents,\n",
        "        HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "\n",
        "    self.retriever = db.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={'k': 4}\n",
        "    )\n",
        "    return self.retriever\n",
        ""
      ],
      "metadata": {
        "id": "Rgo5XCqpe6Jv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ultimate_rag = UltimateRAG(chunk_size=1000, chunk_overlap=200)\n",
        "retriever = ultimate_rag.create_retriever(url_paths=url_paths[:200], dir_path=\"/content/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAVgQ5yhkw4J",
        "outputId": "5c0ad80e-09b8-48a9-edf7-1775232daf48"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 17601, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 16353, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 9648, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4836, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1805, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 8191, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2045, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4751, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4333, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 37561, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 20678, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 8135, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 8444, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1199, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 19423, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 19241, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 19095, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 19743, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 21881, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 21507, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 20721, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 19889, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2865, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 9511, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 6983, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1083, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1346, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1213, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1053, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1269, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1213, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1015, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1279, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2060, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 4941, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2262, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1309, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: page_content=\"# The One Where Monica Gets a New Roommate (The Pilot-The Uncut Version)\\n\\n* * *\\n\\nWritten by: Marta Kauffman & David Crane  \\nTranscribed by: guineapig  \\nAdditional transcribing by: Eric Aasen  \\n(Note: The previously unseen parts of this episode are shown in blue text.)\\n\\n* * *\\n\\n****\\n\\n****[Scene: Central Perk, Chandler, Joey, Phoebe, and Monica are there.]\\n\\n**Monica:** There's nothing to tell! He's just some guy I work with!\\n\\n**Joey:** C'mon, you're going out with the guy! There's gotta be something\\nwrong with him!\\n\\n**Chandler:** All right Joey, be nice.  So does he have a hump? A hump and a\\nhairpiece?\\n\\n**Phoebe:** Wait, does he eat chalk?\\n\\n(They all stare, bemused.)\\n\\n**Phoebe:** Just, 'cause, I don't want her to go through what I went through\\nwith Carl- oh!\\n\\n**Monica:** Okay, everybody relax. This is not even a date. It's just two\\npeople going out to dinner and- not having sex.\\n\\n**Chandler:** Sounds like a date to me.\\n\\n[Time Lapse]\" metadata={'source': 'https://edersoncorbari.github.io/friends-scripts/season/0101.html'}\n",
            "DEBUG: page_content='Scene: A corridor at a sperm bank.\\nSheldon: So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.\\nLeonard: Agreed, what’s your point?\\nSheldon: There’s no point, I just think it’s a good idea for a tee-shirt. \\nLeonard: Excuse me?\\nReceptionist: Hang on. \\nLeonard: One across is Aegean, eight down is Nabakov, twenty-six across is MCM, fourteen down is… move your finger… phylum, which makes fourteen across Port-au-Prince. See, Papa Doc’s capital idea, that’s Port-au-Prince. Haiti. \\nReceptionist: Can I help you?\\nLeonard: Yes. Um, is this the High IQ sperm bank?\\nReceptionist: If you have to ask, maybe you shouldn’t be here.\\nSheldon: I think this is the place.\\nReceptionist: Fill these out.\\nLeonard: Thank-you. We’ll be right back.\\nReceptionist: Oh, take your time. I’ll just finish my crossword puzzle. Oh wait.\\n(They sit and begin to fill in forms).\\nSheldon: Leonard, I don’t think I can do this.\\nLeonard: What, are you kidding? You’re a semi-pro. \\nSheldon: No. We are committing genetic fraud. There’s no guarantee that our sperm is going to generate high IQ offspring, think about that. I have a sister with the same basic DNA mix who hostesses at Fuddruckers.\\nLeonard: Sheldon, this was your idea. A little extra money to get fractional T1 bandwidth in the apartment.\\nSheldon: I know, and I do yearn for faster downloads, but there’s some poor woman is going to pin her hopes on my sperm, what if she winds up with a toddler who doesn’t know if he should use an integral or a differential to solve the area under a curve.\\nLeonard: I’m sure she’ll still love him.\\nSheldon: I wouldn’t.\\nLeonard: Well, what do you want to do?\\nSheldon: I want to leave.\\nLeonard: Okay.\\nSheldon: What’s the protocol for leaving?\\nLeonard: I don’t know, I’ve never reneged on a proffer of sperm before.\\nSheldon: Let’s try just walking out.\\nLeonard: Okay.'\n",
            "DEBUG: page_content='TR\\nƯ\\nỜNG ĐẠI HỌC T\\nÀI CHÍNH \\n–\\n \\nM\\nARKETING\\n \\nB\\nỘ MÔN TOÁN\\n \\nTH\\nỐNG K\\nÊ\\n \\n \\n \\n \\n \\nGiáo Trình\\n \\nLÝ THUY\\nẾT XÁC SUẤT V\\nÀ \\nTH\\nỐNG K\\nÊ \\nỨNG DỤNG\\n \\n(\\nDành cho chương tr\\nình ch\\nất l\\nư\\nợng cao\\n)\\n \\n \\nMã s\\nố : GT \\n–\\n \\n15\\n \\n–\\n \\n21\\n  \\n \\n \\n                                 \\nNhóm biên so\\nạn:\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\nNguy\\nễn Huy Ho\\nàng (Ch\\nủ bi\\nên)\\n \\n                                       \\nNguy\\nễn Trung Đông\\n \\n                                     \\nNguy\\nễn Văn Phong\\n \\n                                             \\nDương Th\\nị Ph\\nương Liên\\n \\n                                   \\nNguy\\nễn Tuấn Duy\\n \\n                \\n                  \\nVõ Th\\nị Bích Khu\\nê\\n \\n   \\n    \\n \\n \\n \\nTHÀNH\\n \\nPH\\nỐ HỒ CHÍ MINH \\n-\\n \\n2021' metadata={'source': '/content/data/giáo_trình_lý_thuyết_xác_suất_và_thống_kê_ứng_dụng.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import getsizeof\n",
        "getsizeof(retriever)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS64T3SCqXwV",
        "outputId": "42a87100-0f6c-4174-a933-08307350f0c5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flYz4WlpsWb5",
        "outputId": "fd262a16-4ef2-47ee-c99e-a5c5383bfdab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7992c6c1e6e0>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}