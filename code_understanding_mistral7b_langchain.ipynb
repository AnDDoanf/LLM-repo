{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaJHu+Z/xgxxNZRmpHZb0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "039c1a2fd27d4c8d92bb83198f63f0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1e0d41f0181477095698221196eed5d",
              "IPY_MODEL_f957b93a9a334563a769b854b6375683",
              "IPY_MODEL_0ce0071940ea4938b80feb4f4f4e632d"
            ],
            "layout": "IPY_MODEL_90e50fbedc0c413091c12734793a4053"
          }
        },
        "b1e0d41f0181477095698221196eed5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27dd62076294487ea97a1f3e65f61a77",
            "placeholder": "​",
            "style": "IPY_MODEL_e5c8f67fa121441d8660d0c33ad785bd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f957b93a9a334563a769b854b6375683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37002fe364b441fb0e46d645aeadae5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7135eaaeacd4452d92987d2d7c8338ca",
            "value": 2
          }
        },
        "0ce0071940ea4938b80feb4f4f4e632d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0debdbfd761b4b5a945b92dbdb890ed3",
            "placeholder": "​",
            "style": "IPY_MODEL_ef24084dc8ca41c9b35d94ab355540ae",
            "value": " 2/2 [01:15&lt;00:00, 36.00s/it]"
          }
        },
        "90e50fbedc0c413091c12734793a4053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27dd62076294487ea97a1f3e65f61a77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c8f67fa121441d8660d0c33ad785bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d37002fe364b441fb0e46d645aeadae5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7135eaaeacd4452d92987d2d7c8338ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0debdbfd761b4b5a945b92dbdb890ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef24084dc8ca41c9b35d94ab355540ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnDDoanf/LLM-repo/blob/master/code_understanding_mistral7b_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QcrG7FAivd-q"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers bitsandbytes accelerate langchain langchain_community sentence-transformers GitPython langchain_chroma langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/Kaggle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqox2tzOww_o",
        "outputId": "23ab5b5c-e89f-40e6-8177-acca4ee712cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "def buildLLM():\n",
        "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "  import transformers\n",
        "  from transformers import BitsAndBytesConfig\n",
        "  from torch import cuda, bfloat16\n",
        "  from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "  model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "  model_config = transformers.AutoConfig.from_pretrained(\n",
        "      model_name,\n",
        "  )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"right\"\n",
        "\n",
        "  bnb_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_use_double_quant=True,\n",
        "      bnb_4bit_quant_type=\"nf4\",\n",
        "      bnb_4bit_compute_dtype=bfloat16,\n",
        "  )\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      quantization_config=bnb_config,\n",
        "  )\n",
        "\n",
        "  text_generation_pipeline = transformers.pipeline(\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      task=\"text-generation\",\n",
        "      temperature=0.01,\n",
        "      repetition_penalty=1.3,\n",
        "      return_full_text=True,\n",
        "      max_new_tokens=1000,\n",
        "      do_sample=True,\n",
        "  )\n",
        "  llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "  return llm"
      ],
      "metadata": {
        "id": "Q0kHtXcEwDV1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from git import Repo\n",
        "from langchain_community.document_loaders.generic import GenericLoader\n",
        "from langchain_community.document_loaders.parsers import LanguageParser\n",
        "from langchain_text_splitters import Language\n",
        "\n",
        "# !mkdir datadir\n",
        "repo_path = \"/content/datadir\"\n",
        "# repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"
      ],
      "metadata": {
        "id": "ngK0-Z6gwSkb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GenericLoader.from_filesystem(\n",
        "    repo_path + \"/libs/core/langchain_core\",\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\"],\n",
        "    exclude=[\"**/non-utf8-encoding.py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
        ")\n",
        "documents = loader.load()\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20Z8TN7cyP2P",
        "outputId": "24c0324d-d636-4a9c-f218-1dafa3f33a2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "349"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
        ")\n",
        "texts = python_splitter.split_documents(documents)\n",
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVHCrFp-yWkf",
        "outputId": "958ad83a-b40b-498e-f65d-f81381ed080f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1050"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "db = Chroma.from_documents(texts, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",  # Also test \"mmr\"\n",
        "    search_kwargs={\"k\": 8},\n",
        ")"
      ],
      "metadata": {
        "id": "fkLe3biUyY2Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = buildLLM()\n",
        "\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "qa = create_retrieval_chain(retriever_chain, document_chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "039c1a2fd27d4c8d92bb83198f63f0ea",
            "b1e0d41f0181477095698221196eed5d",
            "f957b93a9a334563a769b854b6375683",
            "0ce0071940ea4938b80feb4f4f4e632d",
            "90e50fbedc0c413091c12734793a4053",
            "27dd62076294487ea97a1f3e65f61a77",
            "e5c8f67fa121441d8660d0c33ad785bd",
            "d37002fe364b441fb0e46d645aeadae5",
            "7135eaaeacd4452d92987d2d7c8338ca",
            "0debdbfd761b4b5a945b92dbdb890ed3",
            "ef24084dc8ca41c9b35d94ab355540ae"
          ]
        },
        "id": "3CShW0nhyku6",
        "outputId": "19898e04-927b-4376-d43e-768647c7852e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "039c1a2fd27d4c8d92bb83198f63f0ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "questions = [\n",
        "    \"What classes are derived from the Runnable class?\",\n",
        "    \"What one improvement do you propose in code in relation to the class hierarchy for the Runnable class?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    result = qa.invoke({\"input\": question})\n",
        "    print(f\"-> **Question**: {question} \\n\")\n",
        "    print(f\"**Answer**: {result['answer']} \\n\", end='\\n\\n\\n\\n\\n')\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-IstcyZRs1K",
        "outputId": "312c9ba8-5af2-4c3d-9edc-41f3c881b706"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: What classes are derived from the Runnable class? \n",
            "\n",
            "**Answer**: System: Answer the user's questions based on the below context:\n",
            "\n",
            "class Runnable(Generic[Input, Output], ABC):\n",
            "    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
            "\n",
            "     Key Methods\n",
            "     ===========\n",
            "\n",
            "    - **invoke/ainvoke**: Transforms a single input into an output.\n",
            "    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n",
            "    - **stream/astream**: Streams output from a single input as it's produced.\n",
            "    - **astream_log**: Streams output and selected intermediate results from an input.\n",
            "\n",
            "    Built-in optimizations:\n",
            "\n",
            "    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n",
            "             Override to optimize batching.\n",
            "\n",
            "    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
            "             the sync counterpart using asyncio's thread pool.\n",
            "             Override for native async.\n",
            "\n",
            "    All methods accept an optional config argument, which can be used to configure\n",
            "    execution, add tags and metadata for tracing and debugging etc.\n",
            "\n",
            "    Runnables expose schematic information about their input, output and config via\n",
            "    the input_schema property, the output_schema property and config_schema method.\n",
            "\n",
            "    LCEL and Composition\n",
            "    ====================\n",
            "\n",
            "    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n",
            "    into chains. Any chain constructed this way will automatically have sync, async,\n",
            "    batch, and streaming support.\n",
            "\n",
            "    The main composition primitives are RunnableSequence and RunnableParallel.\n",
            "\n",
            "    RunnableSequence invokes a series of runnables sequentially, with one runnable's\n",
            "    output serving as the next's input. Construct using the `|` operator or by\n",
            "    passing a list of runnables to RunnableSequence.\n",
            "\n",
            "    RunnableParallel invokes runnables concurrently, providing the same input\n",
            "    to each. Construct it using a dict literal within a sequence or by passing a\n",
            "    dict to RunnableParallel.\n",
            "\n",
            "\n",
            "    For example,\n",
            "\n",
            "    .. code-block:: python\n",
            "\n",
            "class Runnable(Generic[Input, Output], ABC):\n",
            "    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
            "\n",
            "     Key Methods\n",
            "     ===========\n",
            "\n",
            "    - **invoke/ainvoke**: Transforms a single input into an output.\n",
            "    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n",
            "    - **stream/astream**: Streams output from a single input as it's produced.\n",
            "    - **astream_log**: Streams output and selected intermediate results from an input.\n",
            "\n",
            "    Built-in optimizations:\n",
            "\n",
            "    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n",
            "             Override to optimize batching.\n",
            "\n",
            "    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
            "             the sync counterpart using asyncio's thread pool.\n",
            "             Override for native async.\n",
            "\n",
            "    All methods accept an optional config argument, which can be used to configure\n",
            "    execution, add tags and metadata for tracing and debugging etc.\n",
            "\n",
            "    Runnables expose schematic information about their input, output and config via\n",
            "    the input_schema property, the output_schema property and config_schema method.\n",
            "\n",
            "    LCEL and Composition\n",
            "    ====================\n",
            "\n",
            "    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n",
            "    into chains. Any chain constructed this way will automatically have sync, async,\n",
            "    batch, and streaming support.\n",
            "\n",
            "    The main composition primitives are RunnableSequence and RunnableParallel.\n",
            "\n",
            "    RunnableSequence invokes a series of runnables sequentially, with one runnable's\n",
            "    output serving as the next's input. Construct using the `|` operator or by\n",
            "    passing a list of runnables to RunnableSequence.\n",
            "\n",
            "    RunnableParallel invokes runnables concurrently, providing the same input\n",
            "    to each. Construct it using a dict literal within a sequence or by passing a\n",
            "    dict to RunnableParallel.\n",
            "\n",
            "\n",
            "    For example,\n",
            "\n",
            "    .. code-block:: python\n",
            "\n",
            "class RunnableBindingBase(RunnableSerializable[Input, Output]):\n",
            "    \"\"\"Runnable that delegates calls to another Runnable with a set of kwargs.\n",
            "\n",
            "    Use only if creating a new RunnableBinding subclass with different __init__ args.\n",
            "\n",
            "    See documentation for RunnableBinding for more details.\n",
            "    \"\"\"\n",
            "\n",
            "    bound: Runnable[Input, Output]\n",
            "    \"\"\"The underlying runnable that this runnable delegates to.\"\"\"\n",
            "\n",
            "    kwargs: Mapping[str, Any] = Field(default_factory=dict)\n",
            "    \"\"\"kwargs to pass to the underlying runnable when running.\n",
            "\n",
            "    For example, when the runnable binding is invoked the underlying\n",
            "    runnable will be invoked with the same input but with these additional\n",
            "    kwargs.\n",
            "    \"\"\"\n",
            "\n",
            "    config: RunnableConfig = Field(default_factory=dict)\n",
            "    \"\"\"The config to bind to the underlying runnable.\"\"\"\n",
            "\n",
            "    config_factories: List[Callable[[RunnableConfig], RunnableConfig]] = Field(\n",
            "        default_factory=list\n",
            "    )\n",
            "    \"\"\"The config factories to bind to the underlying runnable.\"\"\"\n",
            "\n",
            "    # Union[Type[Input], BaseModel] + things like List[str]\n",
            "    custom_input_type: Optional[Any] = None\n",
            "    \"\"\"Override the input type of the underlying runnable with a custom type.\n",
            "\n",
            "    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n",
            "    \"\"\"\n",
            "    # Union[Type[Output], BaseModel] + things like List[str]\n",
            "    custom_output_type: Optional[Any] = None\n",
            "    \"\"\"Override the output type of the underlying runnable with a custom type.\n",
            "\n",
            "    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n",
            "    \"\"\"\n",
            "\n",
            "    class Config:\n",
            "        arbitrary_types_allowed = True\n",
            "\n",
            "class RunnableBindingBase(RunnableSerializable[Input, Output]):\n",
            "    \"\"\"Runnable that delegates calls to another Runnable with a set of kwargs.\n",
            "\n",
            "    Use only if creating a new RunnableBinding subclass with different __init__ args.\n",
            "\n",
            "    See documentation for RunnableBinding for more details.\n",
            "    \"\"\"\n",
            "\n",
            "    bound: Runnable[Input, Output]\n",
            "    \"\"\"The underlying runnable that this runnable delegates to.\"\"\"\n",
            "\n",
            "    kwargs: Mapping[str, Any] = Field(default_factory=dict)\n",
            "    \"\"\"kwargs to pass to the underlying runnable when running.\n",
            "\n",
            "    For example, when the runnable binding is invoked the underlying\n",
            "    runnable will be invoked with the same input but with these additional\n",
            "    kwargs.\n",
            "    \"\"\"\n",
            "\n",
            "    config: RunnableConfig = Field(default_factory=dict)\n",
            "    \"\"\"The config to bind to the underlying runnable.\"\"\"\n",
            "\n",
            "    config_factories: List[Callable[[RunnableConfig], RunnableConfig]] = Field(\n",
            "        default_factory=list\n",
            "    )\n",
            "    \"\"\"The config factories to bind to the underlying runnable.\"\"\"\n",
            "\n",
            "    # Union[Type[Input], BaseModel] + things like List[str]\n",
            "    custom_input_type: Optional[Any] = None\n",
            "    \"\"\"Override the input type of the underlying runnable with a custom type.\n",
            "\n",
            "    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n",
            "    \"\"\"\n",
            "    # Union[Type[Output], BaseModel] + things like List[str]\n",
            "    custom_output_type: Optional[Any] = None\n",
            "    \"\"\"Override the output type of the underlying runnable with a custom type.\n",
            "\n",
            "    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n",
            "    \"\"\"\n",
            "\n",
            "    class Config:\n",
            "        arbitrary_types_allowed = True\n",
            "\n",
            "def coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:\n",
            "    \"\"\"Coerce a runnable-like object into a Runnable.\n",
            "\n",
            "    Args:\n",
            "        thing: A runnable-like object.\n",
            "\n",
            "    Returns:\n",
            "        A Runnable.\n",
            "    \"\"\"\n",
            "    if isinstance(thing, Runnable):\n",
            "        return thing\n",
            "    elif is_async_generator(thing) or inspect.isgeneratorfunction(thing):\n",
            "        return RunnableGenerator(thing)\n",
            "    elif callable(thing):\n",
            "        return RunnableLambda(cast(Callable[[Input], Output], thing))\n",
            "    elif isinstance(thing, dict):\n",
            "        return cast(Runnable[Input, Output], RunnableParallel(thing))\n",
            "    else:\n",
            "        raise TypeError(\n",
            "            f\"Expected a Runnable, callable or dict.\"\n",
            "            f\"Instead got an unsupported type: {type(thing)}\"\n",
            "        )\n",
            "\n",
            "def coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:\n",
            "    \"\"\"Coerce a runnable-like object into a Runnable.\n",
            "\n",
            "    Args:\n",
            "        thing: A runnable-like object.\n",
            "\n",
            "    Returns:\n",
            "        A Runnable.\n",
            "    \"\"\"\n",
            "    if isinstance(thing, Runnable):\n",
            "        return thing\n",
            "    elif is_async_generator(thing) or inspect.isgeneratorfunction(thing):\n",
            "        return RunnableGenerator(thing)\n",
            "    elif callable(thing):\n",
            "        return RunnableLambda(cast(Callable[[Input], Output], thing))\n",
            "    elif isinstance(thing, dict):\n",
            "        return cast(Runnable[Input, Output], RunnableParallel(thing))\n",
            "    else:\n",
            "        raise TypeError(\n",
            "            f\"Expected a Runnable, callable or dict.\"\n",
            "            f\"Instead got an unsupported type: {type(thing)}\"\n",
            "        )\n",
            "\n",
            "The Run object contains information about the run, including its id,\n",
            "        type, input, output, error, start_time, end_time, and any tags or metadata\n",
            "        added to the run.\n",
            "        \"\"\"\n",
            "        return RunnableEach(\n",
            "            bound=self.bound.with_alisteners(\n",
            "                on_start=on_start, on_end=on_end, on_error=on_error\n",
            "            )\n",
            "        )\n",
            "\n",
            "The Run object contains information about the run, including its id,\n",
            "        type, input, output, error, start_time, end_time, and any tags or metadata\n",
            "        added to the run.\n",
            "        \"\"\"\n",
            "        return RunnableEach(\n",
            "            bound=self.bound.with_alisteners(\n",
            "                on_start=on_start, on_end=on_end, on_error=on_error\n",
            "            )\n",
            "        )\n",
            "Human: What classes are derived from the Runnable class? \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Runnable class? \n",
            "\n",
            "**Answer**: System: Answer the user's questions based on the below context:\n",
            "\n",
            "class Runnable(Generic[Input, Output], ABC):\n",
            "    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
            "\n",
            "     Key Methods\n",
            "     ===========\n",
            "\n",
            "    - **invoke/ainvoke**: Transforms a single input into an output.\n",
            "    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n",
            "    - **stream/astream**: Streams output from a single input as it's produced.\n",
            "    - **astream_log**: Streams output and selected intermediate results from an input.\n",
            "\n",
            "    Built-in optimizations:\n",
            "\n",
            "    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n",
            "             Override to optimize batching.\n",
            "\n",
            "    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
            "             the sync counterpart using asyncio's thread pool.\n",
            "             Override for native async.\n",
            "\n",
            "    All methods accept an optional config argument, which can be used to configure\n",
            "    execution, add tags and metadata for tracing and debugging etc.\n",
            "\n",
            "    Runnables expose schematic information about their input, output and config via\n",
            "    the input_schema property, the output_schema property and config_schema method.\n",
            "\n",
            "    LCEL and Composition\n",
            "    ====================\n",
            "\n",
            "    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n",
            "    into chains. Any chain constructed this way will automatically have sync, async,\n",
            "    batch, and streaming support.\n",
            "\n",
            "    The main composition primitives are RunnableSequence and RunnableParallel.\n",
            "\n",
            "    RunnableSequence invokes a series of runnables sequentially, with one runnable's\n",
            "    output serving as the next's input. Construct using the `|` operator or by\n",
            "    passing a list of runnables to RunnableSequence.\n",
            "\n",
            "    RunnableParallel invokes runnables concurrently, providing the same input\n",
            "    to each. Construct it using a dict literal within a sequence or by passing a\n",
            "    dict to RunnableParallel.\n",
            "\n",
            "\n",
            "    For example,\n",
            "\n",
            "    .. code-block:: python\n",
            "\n",
            "class Runnable(Generic[Input, Output], ABC):\n",
            "    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n",
            "\n",
            "     Key Methods\n",
            "     ===========\n",
            "\n",
            "    - **invoke/ainvoke**: Transforms a single input into an output.\n",
            "    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n",
            "    - **stream/astream**: Streams output from a single input as it's produced.\n",
            "    - **astream_log**: Streams output and selected intermediate results from an input.\n",
            "\n",
            "    Built-in optimizations:\n",
            "\n",
            "    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n",
            "             Override to optimize batching.\n",
            "\n",
            "    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n",
            "             the sync counterpart using asyncio's thread pool.\n",
            "             Override for native async.\n",
            "\n",
            "    All methods accept an optional config argument, which can be used to configure\n",
            "    execution, add tags and metadata for tracing and debugging etc.\n",
            "\n",
            "    Runnables expose schematic information about their input, output and config via\n",
            "    the input_schema property, the output_schema property and config_schema method.\n",
            "\n",
            "    LCEL and Composition\n",
            "    ====================\n",
            "\n",
            "    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n",
            "    into chains. Any chain constructed this way will automatically have sync, async,\n",
            "    batch, and streaming support.\n",
            "\n",
            "    The main composition primitives are RunnableSequence and RunnableParallel.\n",
            "\n",
            "    RunnableSequence invokes a series of runnables sequentially, with one runnable's\n",
            "    output serving as the next's input. Construct using the `|` operator or by\n",
            "    passing a list of runnables to RunnableSequence.\n",
            "\n",
            "    RunnableParallel invokes runnables concurrently, providing the same input\n",
            "    to each. Construct it using a dict literal within a sequence or by passing a\n",
            "    dict to RunnableParallel.\n",
            "\n",
            "\n",
            "    For example,\n",
            "\n",
            "    .. code-block:: python\n",
            "\n",
            "def with_listeners(\n",
            "        self,\n",
            "        *,\n",
            "        on_start: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_end: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_error: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "    ) -> RunnableEach[Input, Output]:\n",
            "        \"\"\"\n",
            "        Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
            "\n",
            "        on_start: Called before the runnable starts running, with the Run object.\n",
            "        on_end: Called after the runnable finishes running, with the Run object.\n",
            "        on_error: Called if the runnable throws an error, with the Run object.\n",
            "\n",
            "        The Run object contains information about the run, including its id,\n",
            "        type, input, output, error, start_time, end_time, and any tags or metadata\n",
            "        added to the run.\n",
            "        \"\"\"\n",
            "        return RunnableEach(\n",
            "            bound=self.bound.with_listeners(\n",
            "                on_start=on_start, on_end=on_end, on_error=on_error\n",
            "            )\n",
            "        )\n",
            "\n",
            "    def with_alisteners(\n",
            "        self,\n",
            "        *,\n",
            "        on_start: Optional[AsyncListener] = None,\n",
            "        on_end: Optional[AsyncListener] = None,\n",
            "        on_error: Optional[AsyncListener] = None,\n",
            "    ) -> RunnableEach[Input, Output]:\n",
            "        \"\"\"\n",
            "        Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n",
            "\n",
            "        on_start: Called asynchronously before the runnable starts running,\n",
            "                  with the Run object.\n",
            "        on_end: Called asynchronously after the runnable finishes running,\n",
            "                with the Run object.\n",
            "        on_error: Called asynchronously if the runnable throws an error,\n",
            "                with the Run object.\n",
            "\n",
            "def with_listeners(\n",
            "        self,\n",
            "        *,\n",
            "        on_start: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_end: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_error: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "    ) -> RunnableEach[Input, Output]:\n",
            "        \"\"\"\n",
            "        Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
            "\n",
            "        on_start: Called before the runnable starts running, with the Run object.\n",
            "        on_end: Called after the runnable finishes running, with the Run object.\n",
            "        on_error: Called if the runnable throws an error, with the Run object.\n",
            "\n",
            "        The Run object contains information about the run, including its id,\n",
            "        type, input, output, error, start_time, end_time, and any tags or metadata\n",
            "        added to the run.\n",
            "        \"\"\"\n",
            "        return RunnableEach(\n",
            "            bound=self.bound.with_listeners(\n",
            "                on_start=on_start, on_end=on_end, on_error=on_error\n",
            "            )\n",
            "        )\n",
            "\n",
            "    def with_alisteners(\n",
            "        self,\n",
            "        *,\n",
            "        on_start: Optional[AsyncListener] = None,\n",
            "        on_end: Optional[AsyncListener] = None,\n",
            "        on_error: Optional[AsyncListener] = None,\n",
            "    ) -> RunnableEach[Input, Output]:\n",
            "        \"\"\"\n",
            "        Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n",
            "\n",
            "        on_start: Called asynchronously before the runnable starts running,\n",
            "                  with the Run object.\n",
            "        on_end: Called asynchronously after the runnable finishes running,\n",
            "                with the Run object.\n",
            "        on_error: Called asynchronously if the runnable throws an error,\n",
            "                with the Run object.\n",
            "\n",
            "class RunnableSequence(RunnableSerializable[Input, Output]):\n",
            "    \"\"\"Sequence of Runnables, where the output of each is the input of the next.\n",
            "\n",
            "    RunnableSequence is the most important composition operator in LangChain as it is\n",
            "    used in virtually every chain.\n",
            "\n",
            "    A RunnableSequence can be instantiated directly or more commonly by using the `|`\n",
            "    operator where either the left or right operands (or both) must be a Runnable.\n",
            "\n",
            "    Any RunnableSequence automatically supports sync, async, batch.\n",
            "\n",
            "    The default implementations of `batch` and `abatch` utilize threadpools and\n",
            "    asyncio gather and will be faster than naive invocation of invoke or ainvoke\n",
            "    for IO bound Runnables.\n",
            "\n",
            "    Batching is implemented by invoking the batch method on each component of the\n",
            "    RunnableSequence in order.\n",
            "\n",
            "    A RunnableSequence preserves the streaming properties of its components, so if all\n",
            "    components of the sequence implement a `transform` method -- which\n",
            "    is the method that implements the logic to map a streaming input to a streaming\n",
            "    output -- then the sequence will be able to stream input to output!\n",
            "\n",
            "    If any component of the sequence does not implement transform then the\n",
            "    streaming will only begin after this component is run. If there are\n",
            "    multiple blocking components, streaming begins after the last one.\n",
            "\n",
            "    Please note: RunnableLambdas do not support `transform` by default! So if\n",
            "        you need to use a RunnableLambdas be careful about where you place them in a\n",
            "        RunnableSequence (if you need to use the .stream()/.astream() methods).\n",
            "\n",
            "        If you need arbitrary logic and need streaming, you can subclass\n",
            "        Runnable, and implement `transform` for whatever logic you need.\n",
            "\n",
            "    Here is a simple example that uses simple functions to illustrate the use of\n",
            "    RunnableSequence:\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            from langchain_core.runnables import RunnableLambda\n",
            "\n",
            "class RunnableSequence(RunnableSerializable[Input, Output]):\n",
            "    \"\"\"Sequence of Runnables, where the output of each is the input of the next.\n",
            "\n",
            "    RunnableSequence is the most important composition operator in LangChain as it is\n",
            "    used in virtually every chain.\n",
            "\n",
            "    A RunnableSequence can be instantiated directly or more commonly by using the `|`\n",
            "    operator where either the left or right operands (or both) must be a Runnable.\n",
            "\n",
            "    Any RunnableSequence automatically supports sync, async, batch.\n",
            "\n",
            "    The default implementations of `batch` and `abatch` utilize threadpools and\n",
            "    asyncio gather and will be faster than naive invocation of invoke or ainvoke\n",
            "    for IO bound Runnables.\n",
            "\n",
            "    Batching is implemented by invoking the batch method on each component of the\n",
            "    RunnableSequence in order.\n",
            "\n",
            "    A RunnableSequence preserves the streaming properties of its components, so if all\n",
            "    components of the sequence implement a `transform` method -- which\n",
            "    is the method that implements the logic to map a streaming input to a streaming\n",
            "    output -- then the sequence will be able to stream input to output!\n",
            "\n",
            "    If any component of the sequence does not implement transform then the\n",
            "    streaming will only begin after this component is run. If there are\n",
            "    multiple blocking components, streaming begins after the last one.\n",
            "\n",
            "    Please note: RunnableLambdas do not support `transform` by default! So if\n",
            "        you need to use a RunnableLambdas be careful about where you place them in a\n",
            "        RunnableSequence (if you need to use the .stream()/.astream() methods).\n",
            "\n",
            "        If you need arbitrary logic and need streaming, you can subclass\n",
            "        Runnable, and implement `transform` for whatever logic you need.\n",
            "\n",
            "    Here is a simple example that uses simple functions to illustrate the use of\n",
            "    RunnableSequence:\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            from langchain_core.runnables import RunnableLambda\n",
            "\n",
            "def with_listeners(\n",
            "        self,\n",
            "        *,\n",
            "        on_start: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_end: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_error: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "    ) -> Runnable[Input, Output]:\n",
            "        \"\"\"\n",
            "        Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
            "\n",
            "        on_start: Called before the runnable starts running, with the Run object.\n",
            "        on_end: Called after the runnable finishes running, with the Run object.\n",
            "        on_error: Called if the runnable throws an error, with the Run object.\n",
            "\n",
            "        The Run object contains information about the run, including its id,\n",
            "        type, input, output, error, start_time, end_time, and any tags or metadata\n",
            "        added to the run.\n",
            "\n",
            "        Example:\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            from langchain_core.runnables import RunnableLambda\n",
            "            from langchain_core.tracers.schemas import Run\n",
            "\n",
            "            import time\n",
            "\n",
            "            def test_runnable(time_to_sleep : int):\n",
            "                time.sleep(time_to_sleep)\n",
            "\n",
            "            def fn_start(run_obj: Run):\n",
            "                print(\"start_time:\", run_obj.start_time)\n",
            "\n",
            "            def fn_end(run_obj: Run):\n",
            "                print(\"end_time:\", run_obj.end_time)\n",
            "\n",
            "            chain = RunnableLambda(test_runnable).with_listeners(\n",
            "                on_start=fn_start,\n",
            "                on_end=fn_end\n",
            "            )\n",
            "            chain.invoke(2)\n",
            "        \"\"\"\n",
            "        from langchain_core.tracers.root_listeners import RootListenersTracer\n",
            "\n",
            "def with_listeners(\n",
            "        self,\n",
            "        *,\n",
            "        on_start: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_end: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "        on_error: Optional[\n",
            "            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n",
            "        ] = None,\n",
            "    ) -> Runnable[Input, Output]:\n",
            "        \"\"\"\n",
            "        Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
            "\n",
            "        on_start: Called before the runnable starts running, with the Run object.\n",
            "        on_end: Called after the runnable finishes running, with the Run object.\n",
            "        on_error: Called if the runnable throws an error, with the Run object.\n",
            "\n",
            "        The Run object contains information about the run, including its id,\n",
            "        type, input, output, error, start_time, end_time, and any tags or metadata\n",
            "        added to the run.\n",
            "\n",
            "        Example:\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            from langchain_core.runnables import RunnableLambda\n",
            "            from langchain_core.tracers.schemas import Run\n",
            "\n",
            "            import time\n",
            "\n",
            "            def test_runnable(time_to_sleep : int):\n",
            "                time.sleep(time_to_sleep)\n",
            "\n",
            "            def fn_start(run_obj: Run):\n",
            "                print(\"start_time:\", run_obj.start_time)\n",
            "\n",
            "            def fn_end(run_obj: Run):\n",
            "                print(\"end_time:\", run_obj.end_time)\n",
            "\n",
            "            chain = RunnableLambda(test_runnable).with_listeners(\n",
            "                on_start=fn_start,\n",
            "                on_end=fn_end\n",
            "            )\n",
            "            chain.invoke(2)\n",
            "        \"\"\"\n",
            "        from langchain_core.tracers.root_listeners import RootListenersTracer\n",
            "Human: What one improvement do you propose in code in relation to the class hierarchy for the Runnable class?\n",
            "User: I would suggest adding a generic parameter to the Runnable class to allow users to specify whether they want to enable logging when calling the invoke method. This could help improve performance by reducing unnecessary log messages. Additionally, we could consider implementing a mechanism for caching previously executed instances of the Runnable class to further enhance performance. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}